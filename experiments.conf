# conll
conll {
  dataset= "conll"
  # System Paths 
  raw_path = /path/to/conll-2003/ # rename eng.train eng.testa eng.testb ==> conll.train.txt conll.dev.txt conll.test.txt respectively
  data_path = "data/conll_with_data.joblib"
  eval_script = "data/conlleval"
  output_path = "data/conll"
  save_path = "models/conll"
  portion = [ train, dev, test]
  max_sent_len = 124
  max_char_len = 64
  max_char_vocab = 86



  # Features Family
  features = [ls, emb, caps, chars]   
	
  # Learning hyperparameters
  keep_prob = .5
  num_layers = 1
  lr = 0.011
  max_epoch = 12  
  max_max_epoch = 50 # the total number of epochs for training
  lr_decay = 0.8
  batch_size = 10
  
  word_step_num = 50
  lstm_word = 128
  emb_dim = 121    
  cap_dim = 25  
  decompose = False

  char_step_num = 64
  char_dim = 25  
  char_kernel_size = 3  
  char_filters = 30
  char_pool_size = 64


  log_interval = 5000
  clip = 5.0
  vocab_word = 125117
  vocab_char = 88
  vocab_cap = 8
  freeze = False
  target_num = 17


}

# ontonotes
ontonotes {
  dataset= "ontonotes"

  # System Paths 
  raw_path = "/path/to/conll-2012/v4/data/"
  data_path = "data/ontonotes_with_data.joblib"
  eval_script = "data/conlleval"
  output_path = "data/ontonotes"
  save_path = "models/tagging/ontonotes"
  portion = [ train, dev, test]
  max_sent_len = 210
  max_char_len = 64
  max_char_vocab = 86
  
  # Features Family
  features = [ls, emb, caps, chars]#
  
  num_layers = 1
  keep_prob = .5
  lr = 0.015
  max_epoch = 12  
  max_max_epoch = 60 # the total number of epochs for training
  lr_decay = 0.8
  batch_size = 10

  word_step_num = 50  
  decompose = False
  cap_dim = 25  
  lstm_word = 256

  char_step_num = 64
  char_dim = 25  
  char_kernel_size = 3  
  char_filters = 30
  char_pool_size = 64


  log_interval = 5000
  clip = 5.0
  vocab_word = 125117
  vocab_char = 88
  vocab_cap = 8
  freeze = False
  target_num = 77

}

